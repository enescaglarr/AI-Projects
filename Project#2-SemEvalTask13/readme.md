# SemEval-2026 Task 13: Detecting Machine-Generated Code

This repository contains the project implementation for **SemEval-2026 Task 13 (Subtask A)**. The goal is to develop a robust binary classification system capable of distinguishing between human-written source code and code generated by large language models (LLMs).

## Team Members
* Barbaros Yahya
* Enes Çağlar
* Filiz Ilgaz Sönmez
* İsa Utku Dursunoğlu
* Mehmet Barış Baştuğ

---

## Project Overview
As AI-generated code becomes more prevalent in professional and academic environments, identifying its origin is vital for maintaining security and integrity. This project explores multiple detection strategies, ranging from traditional machine learning with handcrafted structural features to fine-tuned transformer architectures.

### Key Features
* **Multi-language Support:** Trained on C++, Python, and Java datasets.
* **Structural Feature Extraction:** Uses abstract syntax tree (AST) metrics like cyclomatic complexity, nesting depth, and maintainability indices.
* **Semantic Preprocessing:** Implements masking of comments, imports, and variable names to prevent the model from relying on surface-level stylistic artifacts.

---

## Methodology

### 1. Data Processing
To ensure the models learn the underlying logic of the code rather than superficial formatting, we applied:
* **Feature Engineering:** Extraction of 13 specific statistical and structural code metrics.
* **Variable & Literal Masking:** Replacing specific variable names and strings with generic tokens while keeping programming language keywords intact.

### 2. Modeling Approaches
We compared several architectural approaches:
* **Neural Models:** Fine-tuned encoder-based transformers including **ModernBERT**, **CodeBERT**, and **UniXcoder**.
* **Gradient Boosting:** A **CatBoost** classifier trained on structural metrics.
* **Lexical Baselines:** TF-IDF vectorization paired with **Support Vector Machines (SVM)** and **Naive Bayes**.

---

## Repository Contents
* `Group18_final_code.ipynb`: The complete Jupyter Notebook containing data loading, feature extraction, model training, and evaluation logic.
* `Group18_final_report.pdf`: A technical paper detailing the research background, methodology, and comprehensive analysis of results.
* `Group18_final_presentation.pptx`: A slide deck summarizing the project objectives and key findings.

---

## Performance Results

The models were evaluated using the **Macro F1 score** to account for class distribution across different programming languages.

| Model | Validation Macro F1 | Hidden Test Macro F1 |
| :--- | :--- | :--- |
| **ModernBERT** | 0.9970 | 0.3521 |
| **CodeBERT** | 0.9820 | 0.3450 |
| **CatBoost** | 0.9700 | 0.3800 |
| **TF-IDF + SVM** | 0.8500 | **0.4589** |

*Note: While deep learning models excelled on validation data, traditional lexical models showed higher robustness on the unseen hidden test set.*

---

## Getting Started

### Installation
Ensure you have Python 3.13+ installed along with the following dependencies:
```bash
pip install tree-sitter tree-sitter-python tree-sitter-java tree-sitter-cpp tree-sitter-c
pip install catboost transformers scikit-learn pandas matplotlib
